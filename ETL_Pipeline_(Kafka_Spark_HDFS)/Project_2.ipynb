{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 9096k  100 9096k    0     0  15.2M      0 --:--:-- --:--:-- --:--:-- 19.5M\n"
     ]
    }
   ],
   "source": [
    "! curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"project2wwblodge1_default\" with the default driver\n",
      "Creating project2wwblodge1_cloudera_1\n",
      "Creating project2wwblodge1_zookeeper_1\n",
      "Creating project2wwblodge1_mids_1\n",
      "Creating project2wwblodge1_spark_1\n",
      "Creating project2wwblodge1_kafka_1\n"
     ]
    }
   ],
   "source": [
    "! docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name             Command             State              Ports       \n",
      "-------------------------------------------------------------------------\n",
      "project2wwblodge   cdh_startup_scri   Up                 11000/tcp,       \n",
      "1_cloudera_1       pt.sh                                 11443/tcp,       \n",
      "                                                         19888/tcp,       \n",
      "                                                         50070/tcp,       \n",
      "                                                         8020/tcp,        \n",
      "                                                         8088/tcp,        \n",
      "                                                         8888/tcp,        \n",
      "                                                         9090/tcp         \n",
      "project2wwblodge   /etc/confluent/d   Up                 29092/tcp,       \n",
      "1_kafka_1          ocker/run                             9092/tcp         \n",
      "project2wwblodge   /bin/bash          Up                 8888/tcp         \n",
      "1_mids_1                                                                  \n",
      "project2wwblodge   docker-            Up                 0.0.0.0:8888->88 \n",
      "1_spark_1          entrypoint.sh                         88/tcp           \n",
      "                   bash                                                   \n",
      "project2wwblodge   /etc/confluent/d   Up                 2181/tcp,        \n",
      "1_zookeeper_1      ocker/run                             2888/tcp,        \n",
      "                                                         32181/tcp,       \n",
      "                                                         3888/tcp         \n"
     ]
    }
   ],
   "source": [
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2020-10-26 19:38 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "# hadoop cloudera yarn\n",
    "! docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! docker-compose logs -f kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic assessment.\n"
     ]
    }
   ],
   "source": [
    "# create a topic\n",
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessment \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: assessment\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: assessment\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n"
     ]
    }
   ],
   "source": [
    "# check the topic\n",
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "  --describe \\\n",
    "  --topic assessment \\\n",
    "  --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name             Command             State              Ports       \n",
      "-------------------------------------------------------------------------\n",
      "project2wwblodge   cdh_startup_scri   Up                 11000/tcp,       \n",
      "1_cloudera_1       pt.sh                                 11443/tcp,       \n",
      "                                                         19888/tcp,       \n",
      "                                                         50070/tcp,       \n",
      "                                                         8020/tcp,        \n",
      "                                                         8088/tcp,        \n",
      "                                                         8888/tcp,        \n",
      "                                                         9090/tcp         \n",
      "project2wwblodge   /etc/confluent/d   Up                 29092/tcp,       \n",
      "1_kafka_1          ocker/run                             9092/tcp         \n",
      "project2wwblodge   /bin/bash          Up                 8888/tcp         \n",
      "1_mids_1                                                                  \n",
      "project2wwblodge   docker-            Up                 0.0.0.0:8888->88 \n",
      "1_spark_1          entrypoint.sh                         88/tcp           \n",
      "                   bash                                                   \n",
      "project2wwblodge   /etc/confluent/d   Up                 2181/tcp,        \n",
      "1_zookeeper_1      ocker/run                             2888/tcp,        \n",
      "                                                         32181/tcp,       \n",
      "                                                         3888/tcp         \n"
     ]
    }
   ],
   "source": [
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced.\n"
     ]
    }
   ],
   "source": [
    "# cat file, process through jq, kafkacat producer send to broker\n",
    "! docker-compose exec mids bash -c \"cat /w205/project-2-wwblodge1/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessment && echo 'Produced.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat file, process through jq, kafkacat producer send to broker\n",
    "# ! docker-compose exec mids bash -c \"cat /w205/project-2-wwblodge1/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 19:40:05.632 NotebookApp]\u001b(B\u001b[m Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n",
      "\u001b[32m[I 19:40:05.669 NotebookApp]\u001b(B\u001b[m Serving notebooks from local directory: /w205\n",
      "\u001b[32m[I 19:40:05.670 NotebookApp]\u001b(B\u001b[m 0 active kernels \n",
      "\u001b[32m[I 19:40:05.670 NotebookApp]\u001b(B\u001b[m The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=a0068c806467bab43f1738a7c939c7a08675c0f457f666a3\n",
      "\u001b[32m[I 19:40:05.671 NotebookApp]\u001b(B\u001b[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[C 19:40:05.672 NotebookApp] \n",
      "    \n",
      "    Copy/paste this URL into your browser when you connect for the first time,\n",
      "    to login with a token:\n",
      "        http://0.0.0.0:8888/?token=a0068c806467bab43f1738a7c939c7a08675c0f457f666a3\n",
      "\u001b[32m[I 19:40:24.931 NotebookApp]\u001b(B\u001b[m 302 GET /?token=a0068c806467bab43f1738a7c939c7a08675c0f457f666a3 (68.6.175.235) 0.72ms\n",
      "\u001b[32m[I 19:40:34.208 NotebookApp]\u001b(B\u001b[m Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\n",
      "\u001b[33m[W 19:40:34.212 NotebookApp]\u001b(B\u001b[m Notebook project-2-wwblodge1/Project_2_Spark.ipynb is not trusted\n",
      "\u001b[32m[I 19:40:34.735 NotebookApp]\u001b(B\u001b[m Kernel started: bd04ae9d-37f5-4ce5-94b2-2084f435aad8\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "20/10/26 19:40:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\u001b[33m[W 19:40:44.865 NotebookApp]\u001b(B\u001b[m Timeout waiting for kernel_info reply from bd04ae9d-37f5-4ce5-94b2-2084f435aad8\n",
      "20/10/26 19:40:50 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "20/10/26 19:41:34 WARN NetworkClient: Error while fetching metadata with correlation id 1 : {commits=LEADER_NOT_AVAILABLE}\n",
      "\u001b[32m[I 19:42:25.256 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-wwblodge1/Project_2_Spark.ipynb\n"
     ]
    }
   ],
   "source": [
    "# instead of running python, run ipynb\n",
    "! docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root --notebook-dir=/w205/' pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name             Command             State              Ports       \n",
      "-------------------------------------------------------------------------\n",
      "project2wwblodge   cdh_startup_scri   Up                 11000/tcp,       \n",
      "1_cloudera_1       pt.sh                                 11443/tcp,       \n",
      "                                                         19888/tcp,       \n",
      "                                                         50070/tcp,       \n",
      "                                                         8020/tcp,        \n",
      "                                                         8088/tcp,        \n",
      "                                                         8888/tcp,        \n",
      "                                                         9090/tcp         \n",
      "project2wwblodge   /etc/confluent/d   Up                 29092/tcp,       \n",
      "1_kafka_1          ocker/run                             9092/tcp         \n",
      "project2wwblodge   /bin/bash          Up                 8888/tcp         \n",
      "1_mids_1                                                                  \n",
      "project2wwblodge   docker-            Up                 0.0.0.0:8888->88 \n",
      "1_spark_1          entrypoint.sh                         88/tcp           \n",
      "                   bash                                                   \n",
      "project2wwblodge   /etc/confluent/d   Up                 2181/tcp,        \n",
      "1_zookeeper_1      ocker/run                             2888/tcp,        \n",
      "                                                         32181/tcp,       \n",
      "                                                         3888/tcp         \n"
     ]
    }
   ],
   "source": [
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping project2wwblodge1_spark_1 ... \n",
      "Stopping project2wwblodge1_kafka_1 ... \n",
      "Stopping project2wwblodge1_cloudera_1 ... \n",
      "Stopping project2wwblodge1_zookeeper_1 ... \n",
      "Stopping project2wwblodge1_mids_1 ... \n",
      "\u001b[2Bping project2wwblodge1_zookeeper_1 ... done[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K^C\n",
      "\u001b[31mERROR\u001b[0m: Aborting.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
